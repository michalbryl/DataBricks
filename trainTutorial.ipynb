{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24c4cd06-deaf-4946-a77a-b4df55a7eed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Read the AccuWeather hourly dataset from the samples catalog\n",
    "dfDaily = spark.read.table(\"samples.accuweather.forecast_daily_calendar_metric\")\n",
    "dfHistorical = spark.read.table(\"samples.accuweather.historical_hourly_metric\")\n",
    "# Or if you prefer, use display if you're in a notebook\n",
    "#display(df)\n",
    "#df.groupBy(\"city_name\").count().orderBy(\"count\", ascending=False).display()\n",
    "df_hongkong = dfDaily.filter(col(\"city_name\") == \"hong kong\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ac85f3f-2231-4b0f-87e4-53d2840f8f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM samples.accuweather.forecast_daily_calendar_metric\n",
    "where city_name = 'hong kong'\n",
    "Limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50464f77-71af-4ed7-b006-5af9603c638b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfSales = spark.read.table(\"samples.bakehouse.sales_customers\")\n",
    "dfSalesTransactions = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "dfSalesFranchise = spark.read.table(\"samples.bakehouse.sales_franchises\")\n",
    "#dfSalesTransactions.show(5)\n",
    "#dfSales.show(5) \n",
    "#dfSalesFranchise.show(5)\n",
    "\n",
    "dfSalesTransactions.join(dfSalesFranchise, dfSalesTransactions['franchiseId'] == dfSalesFranchise['franchiseId']).groupBy(dfSalesFranchise['name']).count().orderBy('count', ascending=False).show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "trainTutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
